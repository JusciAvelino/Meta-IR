{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rb8SlXqhcxg",
        "outputId": "223296b3-87dd-4685-d573-cad782efc7d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: smogn in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: resreg in /usr/local/lib/python3.10/dist-packages (0.2)\n",
            "Requirement already satisfied: ImbalancedLearningRegression in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from smogn) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from smogn) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from smogn) (4.66.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from resreg) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from resreg) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->smogn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->smogn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->smogn) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->resreg) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->resreg) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->smogn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install smogn resreg ImbalancedLearningRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GMmQIsUDaHy5"
      },
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import LeaveOneOut, RepeatedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Rpy2 (R bindings for Python)\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr, SignatureTranslatedAnonymousPackage\n",
        "from rpy2.robjects import default_converter, pandas2ri\n",
        "from rpy2.robjects.conversion import Converter, localconverter\n",
        "import rpy2.robjects.numpy2ri\n",
        "from rpy2.robjects.vectors import StrVector\n",
        "\n",
        "# Other libraries\n",
        "import smogn\n",
        "import resreg\n",
        "import itertools as it\n",
        "from glob import glob\n",
        "import ImbalancedLearningRegression as iblr\n",
        "import os\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Activate converters and filters\n",
        "rpy2.robjects.numpy2ri.activate()\n",
        "pandas2ri.activate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q1RwBqv2Z-PW"
      },
      "outputs": [],
      "source": [
        "class META_IR():\n",
        "\n",
        "    \"\"\"\n",
        "    A class for meta-learning in information retrieval.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : pandas.DataFrame\n",
        "        The dataset.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    m : pandas.DataFrame\n",
        "        The meta-base dataset.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    meta_feature_extraction(self)\n",
        "        Extract Meta-features.\n",
        "    meta_target_definition(self)\n",
        "        Define meta-targets.\n",
        "    balance(self, train, strategy, c)\n",
        "        Data balancing.\n",
        "    install_rpackages\n",
        "        Install R packages\n",
        "    scores(y, y_test, y_pred)\n",
        "        Regression Evaluation\n",
        "    repeatedKfold(self, X, y, dataset, n_splits=10, n_repeats=2, random_state=42, pipeline=None, param_grid=None) :\n",
        "        Evaluation pipelines\n",
        "    pipe_generation(self)\n",
        "        Pipeline Generation\n",
        "    select_best(self, df)\n",
        "        Selects the best pipeline for the dataset\n",
        "    evalutation(y_true, y_pred)\n",
        "        Evaluate the performance of the model.\n",
        "    train(x_train, y_train)\n",
        "        Train the meta-model.\n",
        "    prediction(model, x_test)\n",
        "        Make predictions using the meta-model.\n",
        "    generation(X, Y)\n",
        "        Generate predictions using leave-one-out cross-validation.\n",
        "    independent_training(self)\n",
        "        Train and evaluate the model independently for each label.\n",
        "    model_first(self)\n",
        "        Train the model first, then the strategy.\n",
        "    strategy_first(self)\n",
        "        Train the strategy first, then the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_sets):\n",
        "      self.data_sets = data_sets\n",
        "\n",
        "    def meta_feature_extraction(self):\n",
        "\n",
        "      if not hasattr(self, 'data_sets') or not self.data_sets:\n",
        "          raise ValueError(\"O atributo 'self.data_sets' não foi definido ou está vazio.\")\n",
        "\n",
        "      r_data_sets = StrVector(self.data_sets)\n",
        "\n",
        "      string = \"\"\"\n",
        "      ecol <- function(data_sets){\n",
        "        library(ECoL)\n",
        "        library(UBL)\n",
        "\n",
        "        result_list <- list()\n",
        "\n",
        "        for (i in data_sets) {\n",
        "            print(i)\n",
        "            ds <- read.csv(i)\n",
        "            ds_n <- basename(i)\n",
        "\n",
        "            l <- linearity(target~ ., ds, summary=c(\"mean\", \"min\", \"max\", \"sd\"))\n",
        "            d <- dimensionality(target~ ., ds, summary=c(\"mean\", \"min\", \"max\", \"sd\"))\n",
        "            c <- correlation(target~ ., ds, summary=c(\"mean\", \"min\", \"max\", \"sd\"))\n",
        "            s <- smoothness(target~ ., ds, summary=c(\"mean\", \"min\", \"max\", \"sd\"))\n",
        "\n",
        "            y <- ds$target\n",
        "            if (sum(is.na(y)) == 0){\n",
        "                pc <- UBL::phi.control(y)\n",
        "                y.phi <- phi(y, pc)\n",
        "\n",
        "                n_raro <- sum(y.phi > 0.8)\n",
        "                n_row <- nrow(ds)\n",
        "                n_col <- ncol(ds) - 1\n",
        "                p_raro <- ((n_raro / n_row) * 100)\n",
        "            } else {\n",
        "                n_raro <- NA\n",
        "                n_row <- nrow(ds)\n",
        "                n_col <- ncol(ds) - 1\n",
        "                p_raro <- NA\n",
        "            }\n",
        "\n",
        "            myList <- list(n_raro=n_raro, n_row=n_row, n_col=n_col, p_raro=p_raro, l=l, d=d, c=c, s=s)\n",
        "            temp_df <- data.frame(matrix(unlist(myList), nrow = 1), stringsAsFactors = FALSE)\n",
        "            result_list[[length(result_list) + 1]] <- temp_df\n",
        "        }\n",
        "\n",
        "        result_df <- do.call(rbind, result_list)\n",
        "        return(result_df)\n",
        "    }\n",
        "    \"\"\"\n",
        "      powerpack = SignatureTranslatedAnonymousPackage(string, \"powerpack\")\n",
        "      df = powerpack.ecol(r_data_sets)\n",
        "\n",
        "      return df\n",
        "\n",
        "    def meta_target_definition(self):\n",
        "\n",
        "      pipes_params = self.pipe_generation()\n",
        "\n",
        "      all_results = []\n",
        "      for i, dataset in enumerate(self.data_sets):\n",
        "\n",
        "        ds = pd.read_csv(dataset)\n",
        "        path = dataset\n",
        "        head, tail = os.path.split(path)\n",
        "        print(\"=====================\")\n",
        "        print(path)\n",
        "\n",
        "        X = ds.drop([ds.columns[0]], axis = 1)\n",
        "        y = ds[ds.columns[0]]\n",
        "\n",
        "        X = X.to_numpy()\n",
        "        y = y.to_numpy()\n",
        "\n",
        "        models_results = []\n",
        "        for j in pipes_params:\n",
        "\n",
        "          pipeline, param_grid = j[0], j[1]\n",
        "          print(str(pipeline.steps[0][1]).split('(')[0])\n",
        "          models_results.append(self.repeatedKfold(X=X, y=y, dataset=dataset, pipeline=pipeline, param_grid=param_grid))\n",
        "        models_results = pd.concat(models_results).reset_index(drop=True)\n",
        "        best_results = self.select_best(models_results)\n",
        "        all_results.append(best_results)\n",
        "      all_results = pd.concat(all_results)\n",
        "      return all_results\n",
        "\n",
        "    def balance(self, train, strategy, c):\n",
        "\n",
        "      if strategy == \"GN\":\n",
        "        train = iblr.gn(data = train, y = train.columns[0],  rel_thres = 0.8)\n",
        "      elif strategy == \"RO\":\n",
        "        train = iblr.ro(data = train, y = train.columns[0], rel_thres = 0.8)\n",
        "      elif strategy == \"RU\":\n",
        "        train = iblr.random_under(data = train, y = train.columns[0], rel_thres = 0.8)\n",
        "      elif strategy == \"SG\":\n",
        "        train =  train.dropna()\n",
        "        train = smogn.smoter(data = train, y = train.columns[0], rel_xtrm_type = 'high', rel_thres = 0.8)\n",
        "        train =  train.dropna()\n",
        "      elif strategy == \"SMT\":\n",
        "        train = iblr.smote(data = train, y = train.columns[0], rel_thres = 0.8)\n",
        "      elif strategy == \"WC\":\n",
        "        X_train = train.drop([train.columns[0]], axis = 1)\n",
        "        y_train  = train[train.columns[0]]\n",
        "        relevance = resreg.pdf_relevance(y_train)\n",
        "        X_wercs, y_wercs = resreg.wercs(X_train, y_train, relevance)\n",
        "        trainWC = np.column_stack((y_wercs, X_wercs))\n",
        "        train = pd.DataFrame(trainWC)\n",
        "\n",
        "      return train\n",
        "\n",
        "    def install_rpackages(self):\n",
        "      string = \"\"\"\n",
        "\n",
        "      U1 <- function(){\n",
        "\n",
        "\n",
        "          install.packages(\"ECoL\")\n",
        "          install.packages(\"UBL\")\n",
        "\n",
        "          install.packages(\"devtools\")\n",
        "          library(devtools)\n",
        "\n",
        "          install.packages(c(\"operators\", \"class\", \"fields\", \"ROCR\", \"Hmisc\", \"performanceEstimation\"))\n",
        "\n",
        "          install.packages(c(\"zoo\",\"xts\",\"quantmod\"))\n",
        "\n",
        "          install.packages( \"https://cran.r-project.org/src/contrib/Archive/DMwR/DMwR_0.4.1.tar.gz\", repos=NULL, type=\"source\" )\n",
        "\n",
        "          install.packages(\"IRon\")\n",
        "          install_github(\"rpribeiro/uba\")\n",
        "\n",
        "          library(IRon)\n",
        "          library(uba)\n",
        "\n",
        "      }\n",
        "\n",
        "      \"\"\"\n",
        "      powerpack = SignatureTranslatedAnonymousPackage(string, \"powerpack\")\n",
        "\n",
        "      powerpack.U1()\n",
        "\n",
        "\n",
        "    def scores(self, y, y_test, y_pred):\n",
        "\n",
        "      uba = importr(\"uba\")\n",
        "      iron = importr(\"IRon\")\n",
        "\n",
        "      ph = uba.phi_control(y)\n",
        "      ls = uba.loss_control(y)\n",
        "      sera = iron.sera(y_test, y_pred, phi_trues = uba.phi(y_test,ph))\n",
        "      F1 = uba.util(y_pred, y_test, ph, ls, uba.util_control(umetric=\"Fm\", beta=1, event_thr=0.8))\n",
        "\n",
        "      scores_ = list([F1, sera])\n",
        "      return pd.DataFrame(scores_,\n",
        "                columns = [''],\n",
        "                index = ['f1score', 'sera'])\n",
        "\n",
        "    def repeatedKfold(self, X, y, dataset, n_splits=10, n_repeats=2, random_state=42, pipeline=None, param_grid=None) :\n",
        "      rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
        "      all_result = []\n",
        "\n",
        "      strategys = {\"SG\":{},\n",
        "                  \"RU\":{},\n",
        "                  \"RO\":{},\n",
        "                  \"SMT\":{},\n",
        "                  \"GN\":{},\n",
        "                  \"WC\":{},\n",
        "                  'None': {None}\n",
        "                  }\n",
        "\n",
        "      for strategy in strategys:\n",
        "          data_frame = []\n",
        "          params = strategys[strategy]\n",
        "\n",
        "          keys = sorted(params)\n",
        "\n",
        "          if strategy != \"None\" and params:\n",
        "            combinations = it.product(*(params[Name] for Name in keys))\n",
        "          else:\n",
        "            combinations = ['None']\n",
        "          for c in list(combinations):\n",
        "            score_perc = []\n",
        "            for train_index, test_index in rkf.split(X, y):\n",
        "\n",
        "              X_train, X_test = X[train_index], X[test_index]\n",
        "              y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "              train = np.column_stack((y_train, X_train))\n",
        "              train = pd.DataFrame(train)\n",
        "\n",
        "              if strategy != 'None':\n",
        "                try:\n",
        "                  train = self.balance(train, strategy, c)\n",
        "                except ValueError:\n",
        "                  pass\n",
        "\n",
        "              X_train = train.drop([train.columns[0]], axis = 1)\n",
        "              y_train  = train[train.columns[0]]\n",
        "\n",
        "              X_train = X_train.to_numpy()\n",
        "              y_train = y_train.to_numpy()\n",
        "\n",
        "              grid_search = GridSearchCV(pipeline, cv=rkf, param_grid=param_grid)\n",
        "              grid_search.fit(X_train, y_train)\n",
        "              y_pred  = grid_search.predict(X_test)\n",
        "\n",
        "              path = dataset\n",
        "              head, tail = os.path.split(path)\n",
        "\n",
        "              test = np.column_stack((test_index, y_test))\n",
        "              pred = np.column_stack((test_index, y_pred))\n",
        "\n",
        "\n",
        "              score_perc.append(self.scores(y, y_test, y_pred).T)\n",
        "\n",
        "            df = pd.concat(score_perc)\n",
        "\n",
        "            values = [tail,\n",
        "                      str(df.f1score.mean())+ \"({})\".format(df.f1score.std()),\n",
        "                      str(df.sera.mean())+ \"({})\".format(df.sera.std())]\n",
        "\n",
        "            scores_df = pd.DataFrame([values], columns=[\"dataset\", \"f1score\", \"sera\"])\n",
        "\n",
        "            # if len(keys) > 1:\n",
        "            #   scores_df[keys[0]]=c[0]\n",
        "            #   scores_df[keys[1]]=c[1]\n",
        "            #   scores_df['strategy']=strategy\n",
        "            # else:\n",
        "            #   scores_df[keys[0]]=c[0]\n",
        "            #   scores_df['strategy']=strategy\n",
        "\n",
        "            scores_df['strategy'] = strategy\n",
        "\n",
        "\n",
        "            data_frame.append(scores_df)\n",
        "          data_frame = pd.concat(data_frame)\n",
        "\n",
        "          # data_frame.to_csv('result_{}_{}.csv'.format(strategy, str(pipeline.steps[0][1]).split('(')[0]), index = False)\n",
        "          all_result.append(data_frame)\n",
        "      all_result = pd.concat(all_result).reset_index(drop=True)\n",
        "      all_result['model'] = str(pipeline.steps[0][1]).split('(')[0]\n",
        "\n",
        "      return all_result\n",
        "\n",
        "    def pipe_generation(self):\n",
        "      clf_param = dict()\n",
        "      for clf in [BaggingRegressor(), DecisionTreeRegressor(), MLPRegressor(), RandomForestRegressor(), SVR(), XGBRegressor(verbosity=0)\n",
        "                  ]:\n",
        "          clf_param[str(clf).split('(')[0]] = clf\n",
        "\n",
        "      pipes_params = []\n",
        "\n",
        "      for clf,  param_grid in zip([BaggingRegressor(), DecisionTreeRegressor(), MLPRegressor(), RandomForestRegressor(), SVR(), XGBRegressor(verbosity=0)],\n",
        "\n",
        "\n",
        "                      [{},\n",
        "                       {},\n",
        "                       {},\n",
        "                       {},\n",
        "                       {},\n",
        "                       {}]):\n",
        "\n",
        "        configs = []\n",
        "        clf = str(clf).split('(')[0]\n",
        "        for p in param_grid:\n",
        "            aux = p\n",
        "            for i in param_grid[p]:\n",
        "              aux += '+'+str(i)\n",
        "            clf += '|'+aux\n",
        "        configs.append(clf)\n",
        "\n",
        "        for config in configs:\n",
        "\n",
        "          pipeline = Pipeline([('clf', clf_param[config.split('|')[0]])])\n",
        "          params = config.split('|')\n",
        "\n",
        "          param_grid = {}\n",
        "          t, t1 = len(params), 0\n",
        "          for p in range(len(params)):\n",
        "            values = ()\n",
        "            if len(params[p].split('+')) > 2:\n",
        "              a = params[p].split('+')[1:]\n",
        "              for j in a:\n",
        "                if '0.' in j:\n",
        "                  values += (float(j),)\n",
        "                else:\n",
        "                  values += (int(j),)\n",
        "\n",
        "              param_grid[params[p].split('+')[0]] = values\n",
        "\n",
        "            else:\n",
        "\n",
        "              if t1 == t:\n",
        "                if '0.' in params[p].split('+')[1]:\n",
        "                  param_grid[params[p].split('+')[0]] = [params[p].split('+')[1]]\n",
        "                else:\n",
        "                  param_grid[params[p].split('+')[0]] = [params[p].split('+')[1]]\n",
        "              elif t1 < t:\n",
        "                for l in params[t1].split('+')[1:]:\n",
        "\n",
        "                  if '0.' in l:\n",
        "                    param_grid[params[t1].split('+')[0]] = [float(l)]\n",
        "                  else:\n",
        "                    param_grid[params[t1].split('+')[0]] = [int(l)]\n",
        "\n",
        "            t1 += 1\n",
        "\n",
        "        pipes_params.append([pipeline, param_grid])\n",
        "      return pipes_params\n",
        "\n",
        "    def select_best(self, df):\n",
        "      best_result = []\n",
        "      for metric in ['f1score', 'sera']:\n",
        "        df['value'] = df[metric].str.extract(r'^([\\d.]+)\\(')\n",
        "        df['value'] = df['value'].astype(float)\n",
        "        if metric == 'f1score':\n",
        "          best_value = df['value'].max()\n",
        "        else:\n",
        "          best_value = df['value'].min()\n",
        "        best_value = df.loc[df['value'] == best_value, ['dataset', 'model', 'strategy', metric]].reset_index(drop=True)[:1]\n",
        "        best_value.columns = ['dataset', 'model', 'strategy', 'score']\n",
        "        best_value.insert(3, 'metric', metric)\n",
        "        best_result.append(best_value)\n",
        "      best_result = pd.concat(best_result)\n",
        "      return best_result\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Fit the meta-model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_train : pandas.DataFrame\n",
        "            The training data.\n",
        "        y_train : pandas.Series\n",
        "            The training labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sklearn.ensemble.RandomForestClassifier\n",
        "            The trained meta-model.\n",
        "        \"\"\"\n",
        "        self.meta_model = RandomForestClassifier()\n",
        "        self.meta_model.fit(x_train, y_train)\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\"\n",
        "        Make predictions using the meta-model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : sklearn.ensemble.RandomForestClassifier\n",
        "            The meta-model.\n",
        "        x_test : pandas.DataFrame\n",
        "            The test data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            The predicted labels.\n",
        "        \"\"\"\n",
        "        pred = self.meta_model.predict(x_test)[0]\n",
        "        return pred\n",
        "\n",
        "    def generation(self, X, Y):\n",
        "        \"\"\"\n",
        "        Generate predictions using leave-one-out cross-validation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas.DataFrame\n",
        "            The dataset.\n",
        "        Y : pandas.Series\n",
        "            The labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            The predictions.\n",
        "        \"\"\"\n",
        "\n",
        "        loo = LeaveOneOut()\n",
        "        loo.get_n_splits(X)\n",
        "\n",
        "        y_pred = []\n",
        "        for i, (train_index, test_index) in enumerate(loo.split(X, Y)):\n",
        "            # Split the data into training and test sets.\n",
        "            x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
        "\n",
        "            # Fit the meta-model on the training data.\n",
        "            self.fit(x_train, y_train)\n",
        "\n",
        "            # Make predictions on the test data.\n",
        "            y_pred.append(self.predict(x_test))\n",
        "\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def evaluation(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Evaluate the performance of the model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : pandas.Series\n",
        "            The ground truth labels.\n",
        "        y_pred : pandas.Series\n",
        "            The predicted labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            The evaluation scores.\n",
        "        \"\"\"\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "        precision = precision_score(y_true, y_pred, average='macro')\n",
        "        recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        values = [acc, f1, precision, recall]\n",
        "        return values\n",
        "\n",
        "    def independent_training(self, m):\n",
        "\n",
        "        \"\"\"\n",
        "        Train and evaluate the model independently for each label.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            The evaluation scores for each label.\n",
        "        \"\"\"\n",
        "\n",
        "        # Select the numeric features.\n",
        "        X = m.select_dtypes(exclude=['object'])\n",
        "\n",
        "        # Create a dictionary to map each label to its column name.\n",
        "        y_labels = {'y_r': 'strategy', 'y_l': 'model'}\n",
        "\n",
        "        # Iterate over each label.\n",
        "        predictions_df = pd.DataFrame()\n",
        "\n",
        "        if 'dataset' in m.columns:\n",
        "          predictions_df['dataset'] = m['dataset'].values\n",
        "\n",
        "        for y_label in y_labels:\n",
        "            # Get the ground truth labels for the current label.\n",
        "            y_true = m[[y_labels[y_label]]]\n",
        "\n",
        "            # Generate predictions for the current label.\n",
        "            y_pred = self.generation(X, y_true)\n",
        "\n",
        "              # Add the predictions as a new column in the DataFrame.\n",
        "            predictions_df[y_label] = y_pred\n",
        "\n",
        "        # Print the predictions (optional, for debugging purposes).\n",
        "            print(f\"Predictions for {y_label}: {y_pred}\")\n",
        "\n",
        "        return predictions_df\n",
        "\n",
        "    def model_first(self, m):\n",
        "      \"\"\"\n",
        "      Train the model first, then the strategy.\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      pd.DataFrame\n",
        "          The evaluation scores for each label.\n",
        "      \"\"\"\n",
        "\n",
        "      # Select the features that are not object dtype.\n",
        "      X = m.select_dtypes(exclude=['object'])\n",
        "\n",
        "      predictions_df = pd.DataFrame()\n",
        "\n",
        "      if 'dataset' in m.columns:\n",
        "        predictions_df['dataset'] = m['dataset'].values\n",
        "\n",
        "      # Create a dictionary that maps the label name to the column name.\n",
        "      y_labels = {'y_r': 'strategy', 'y_l': 'model'}\n",
        "\n",
        "      # Get the ground truth labels for the first label.\n",
        "      y_true = m[[y_labels['y_l']]]\n",
        "\n",
        "      # Generate predictions for the first label using leave-one-out cross-validation.\n",
        "      y_pred = self.generation(X, y_true)\n",
        "\n",
        "      model_first_evaluations = []\n",
        "\n",
        "      # Evaluate the predictions for the first label.\n",
        "      model_first_values = self.evaluation(y_true, y_pred)\n",
        "\n",
        "      # Create a DataFrame to store the evaluation scores for the first label.\n",
        "      model_first_values = pd.DataFrame([model_first_values],\n",
        "                                        columns=['acc', 'f1', 'precision', 'recall'])\n",
        "      model_first_values.insert(0, 'label', 'y_l')\n",
        "\n",
        "      model_first_evaluations.append(model_first_values)\n",
        "\n",
        "      predictions_df['y_l'] = y_pred\n",
        "\n",
        "      # Add the predicted labels for the first label to the dataset.\n",
        "      X['y_l'] = y_pred\n",
        "\n",
        "      # Get the ground truth labels for the second label.\n",
        "      y_true = m[[y_labels['y_r']]]\n",
        "\n",
        "\n",
        "      # One-hot encode the predicted labels for the first label.\n",
        "      column_to_encoder = 'y_l'\n",
        "      encoder = OneHotEncoder(sparse_output=False)\n",
        "      encoded_column = encoder.fit_transform(X[[column_to_encoder]])\n",
        "      columns_one_hot = encoder.get_feature_names_out([column_to_encoder])\n",
        "      encoded_df = pd.DataFrame(encoded_column, columns=columns_one_hot)\n",
        "\n",
        "      # Concatenate the one-hot encoded predictions with the original dataset.\n",
        "      X = pd.concat([X.drop(column_to_encoder, axis=1), encoded_df], axis=1)\n",
        "\n",
        "      # Generate predictions for the second label using the updated dataset.\n",
        "      y_pred = self.generation(X, y_true)\n",
        "\n",
        "      # Evaluate the predictions for the first label.\n",
        "      model_first_values = self.evaluation(y_true, y_pred)\n",
        "\n",
        "      predictions_df['y_r'] = y_pred\n",
        "\n",
        "      return predictions_df\n",
        "\n",
        "    def strategy_first(self, m):\n",
        "      \"\"\"\n",
        "      Train the model first, then the strategy.\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      pd.DataFrame\n",
        "          The evaluation scores for each label.\n",
        "      \"\"\"\n",
        "\n",
        "      # Select the features that are not object dtype.\n",
        "      X = m.select_dtypes(exclude=['object'])\n",
        "\n",
        "      predictions_df = pd.DataFrame()\n",
        "\n",
        "      if 'dataset' in m.columns:\n",
        "        predictions_df['dataset'] = m['dataset'].values\n",
        "\n",
        "      # Create a dictionary that maps the label name to the column name.\n",
        "      y_labels = {'y_r': 'strategy', 'y_l': 'model'}\n",
        "\n",
        "      # Get the ground truth labels for the first label.\n",
        "      y_true = m[[y_labels['y_r']]]\n",
        "\n",
        "      # Generate predictions for the first label using leave-one-out cross-validation.\n",
        "      y_pred = self.generation(X, y_true)\n",
        "\n",
        "      predictions_df['y_r'] = y_pred\n",
        "\n",
        "      # Add the predicted labels for the first label to the dataset.\n",
        "      X['y_r'] = y_pred\n",
        "\n",
        "      # Get the ground truth labels for the second label.\n",
        "      y_true = m[[y_labels['y_l']]]\n",
        "\n",
        "      # One-hot encode the predicted labels for the first label.\n",
        "      column_to_encoder = 'y_r'\n",
        "      encoder = OneHotEncoder(sparse_output=False)\n",
        "      encoded_column = encoder.fit_transform(X[[column_to_encoder]])\n",
        "      columns_one_hot = encoder.get_feature_names_out([column_to_encoder])\n",
        "      encoded_df = pd.DataFrame(encoded_column, columns=columns_one_hot)\n",
        "\n",
        "      # Concatenate the one-hot encoded predictions with the original dataset.\n",
        "      X = pd.concat([X.drop(column_to_encoder, axis=1), encoded_df], axis=1)\n",
        "\n",
        "      # Generate predictions for the second label using the updated dataset.\n",
        "      y_pred = self.generation(X, y_true)\n",
        "\n",
        "      predictions_df['y_l'] = y_pred\n",
        "\n",
        "      return predictions_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rUY9OQNlSO4"
      },
      "outputs": [],
      "source": [
        "def run_full_meta_ir_pipeline(folder_path):\n",
        "    \"\"\"\n",
        "    Executes the full META_IR pipeline, including concatenation of meta-targets\n",
        "    and applying independent training, model-first, and strategy-first evaluations.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing the datasets.\n",
        "    \"\"\"\n",
        "    # Load datasets from the folder\n",
        "    print(f\"Loading datasets from folder: {folder_path}\")\n",
        "    data_sets = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
        "    if not data_sets:\n",
        "        raise ValueError(\"No CSV files found in the specified folder.\")\n",
        "\n",
        "    # Instantiate the META_IR class\n",
        "    meta_ir = META_IR(data_sets=data_sets)\n",
        "\n",
        "    #Installs the required R packages for the META_IR pipeline.\n",
        "    print(\"Installing R packages...\")\n",
        "    meta_ir.install_rpackages()\n",
        "    print(\"R packages installed successfully.\")\n",
        "\n",
        "    # Extract meta-features\n",
        "    print(\"Extracting meta-features...\")\n",
        "    meta_features = meta_ir.meta_feature_extraction()\n",
        "\n",
        "    # Define meta-targets\n",
        "    print(\"Defining meta-targets...\")\n",
        "    meta_target = meta_ir.meta_target_definition()\n",
        "\n",
        "    sera_meta_target = meta_target[meta_target['metric'] == 'sera']\n",
        "    f1_meta_target = meta_target[meta_target['metric'] == 'f1score']\n",
        "\n",
        "    print(\"Concatenating meta-targets with meta-features...\")\n",
        "\n",
        "    m_sera = pd.concat(\n",
        "    [sera_meta_target[['dataset', 'model', 'strategy']].reset_index(drop=True),\n",
        "     meta_features.reset_index(drop=True)],\n",
        "    axis=1\n",
        "    )\n",
        "\n",
        "    m_f1 = pd.concat(\n",
        "    [f1_meta_target[['dataset', 'model', 'strategy']].reset_index(drop=True),\n",
        "     meta_features.reset_index(drop=True)],\n",
        "    axis=1\n",
        "    )\n",
        "\n",
        "    # Run independent training\n",
        "    print(\"Running independent training...\")\n",
        "    df_independent_training = meta_ir.independent_training(m_sera)\n",
        "    print(\"Independent training results:\")\n",
        "    print(df_independent_training)\n",
        "\n",
        "    # Run model-first evaluation\n",
        "    print(\"Running model-first evaluation...\")\n",
        "    df_model_first = meta_ir.model_first(m_sera)\n",
        "    print(\"Model-first evaluation results:\")\n",
        "    print(df_model_first)\n",
        "\n",
        "    # Run strategy-first evaluation\n",
        "    print(\"Running strategy-first evaluation...\")\n",
        "    df_strategy_first = meta_ir.strategy_first(m_sera)\n",
        "    print(\"Strategy-first evaluation results:\")\n",
        "    print(df_strategy_first)\n",
        "\n",
        "    print(\"META_IR pipeline execution completed.\")\n",
        "\n",
        "# Path to the folder containing datasets\n",
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/ds_30\"  # Replace with your folder path\n",
        "\n",
        "# Execute the full pipeline\n",
        "run_full_meta_ir_pipeline(folder_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}